# -*- coding: utf-8 -*-
"""model_building_and_evaluation_5E5D_layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jox88khnqb14cGTKokUG_osF-I8i-5id
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data= pd.read_csv(r"/content/drive/MyDrive/updated_cleaned_data.csv")
data.head()

data.info()

df = data.sample(n=10000, random_state=42)

df.info()

model_df = df.copy()

# Specify the relevant columns in the new dataset
relevant_columns = ['document', 'summary']

# Filter the DataFrame to keep only the relevant columns
model_df = model_df[relevant_columns]

# Display the first few rows of the new DataFrame
model_df.head()

#model_df = model_df.rename(columns={'lemmatized_document': 'document', 'lemmatized_summary': 'summary'})

# displaying the first few rows of the updated DataFrame
#model_df.head()

texts = model_df['document'].dropna().tolist()

import os
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments
from torch.utils.data import DataLoader

# loading the pretrained BART tokenizer
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')

# loading the pre-trained BART model
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')

# freezing the first 5 layers of Encoder and Decoder for Transfer Learning
for layer in model.model.encoder.layers[:5]:
    for param in layer.parameters():
        param.requires_grad = False

for layer in model.model.decoder.layers[:5]:
    for param in layer.parameters():
        param.requires_grad = False

# customizing the model for summarization on Reddit dataset
model.config.max_length = 100                                          # max length of generated summaries
model.config.min_length = 20                                           # min length of generated summaries
model.config.no_repeat_ngram_size = 3                                  # to avoid repeating trigrams
model.config.early_stopping = True

# splitting the data into training and validation datasets
from sklearn.model_selection import train_test_split
train_data, eval_data = train_test_split(model_df, test_size=0.2)

import torch
from torch.utils.data import Dataset, DataLoader

class RedditSummaryDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=512):
        self.data = dataframe.reset_index(drop=True)  # Reset index to avoid key errors
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
    # Access the document and summary from the DataFrame
        document = str(self.data.loc[idx, 'document'])
        summary = str(self.data.loc[idx, 'summary'])

    # Tokenize document and summary using the correct tokenizer
        inputs = self.tokenizer(
        document,
        max_length=self.max_length,
        padding='max_length',
        truncation=True,
        return_tensors="pt"
        )
        labels = self.tokenizer(
        summary,
        max_length=self.max_length,
        padding='max_length',
        truncation=True,
        return_tensors="pt"
        ).input_ids

    # Make sure to squeeze the inputs to remove unnecessary dimensions
        inputs['input_ids'] = inputs['input_ids'].squeeze()
        inputs['attention_mask'] = inputs['attention_mask'].squeeze()
        inputs['labels'] = labels.squeeze()  # Ensure labels have the correct shape
        return inputs

# creating datasets for training and evaluation
train_dataset = RedditSummaryDataset(train_data, tokenizer)
eval_dataset = RedditSummaryDataset(eval_data, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=True)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()

# Save the model
model.save_pretrained('/content/drive/MyDrive/bart_5E5D_model')

# Save the tokenizer
tokenizer.save_pretrained('/content/drive/MyDrive/bart_5E5D_tokenizer')

import warnings
import evaluate
from nltk.translate.bleu_score import sentence_bleu
import torch
warnings.filterwarnings("ignore", category=UserWarning)
# Load ROUGE metric
rouge = evaluate.load("rouge")

# Define a function to calculate BLEU score
def calculate_bleu(reference_summary, generated_summary):
    # Tokenize the reference and generated summaries
    reference_tokens = reference_summary.split()  # Assuming reference_summary is a string
    generated_tokens = generated_summary.split()  # Assuming generated_summary is a string

    # Calculate BLEU score
    return sentence_bleu([reference_tokens], generated_tokens)

# Function to evaluate the model on the eval_dataset
def evaluate_model(model, tokenizer, test_loader):
    # Setting the model to evaluation mode
    model.eval()
    generated_summaries = []
    reference_summaries = []
    bleu_scores = []

    # Loop through the test_loader for batch processing
    for batch in test_loader:
        # Move inputs to the device (CPU or GPU)
        input_ids = batch['input_ids'].to(model.device)
        attention_mask = batch['attention_mask'].to(model.device)
        reference_summaries_batch = batch['labels']

        # Generate summaries without gradient calculation
        with torch.no_grad():
            summary_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask,
                                         max_length=100, min_length=20, length_penalty=2.0,
                                         num_beams=4, early_stopping=True)

        # Decode generated summaries and append to the list
        for idx in range(len(summary_ids)):
            generated_summary = tokenizer.decode(summary_ids[idx], skip_special_tokens=True)
            generated_summaries.append(generated_summary)

            # Decode the reference summary and calculate BLEU score
            reference_summary = tokenizer.decode(reference_summaries_batch[idx], skip_special_tokens=True)
            reference_summaries.append(reference_summary)

            bleu_score = calculate_bleu(reference_summary, generated_summary)
            bleu_scores.append(bleu_score)

    # Calculate average BLEU score
    average_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0

    # Calculate the ROUGE scores
    rouge_results = rouge.compute(predictions=generated_summaries, references=reference_summaries)
    return rouge_results, average_bleu_score

# Assuming eval_loader is a DataLoader created from the RedditSummaryDataset
rouge_scores, average_bleu = evaluate_model(model, tokenizer, eval_loader)
print("ROUGE Scores:", rouge_scores)
print("Average BLEU Score:", average_bleu)

"""After freezing 5 layers of the BART model, the training and validation loss values over 3 epochs are as follows:
- **Epoch 1**: Training Loss = 0.3581, Validation Loss = 0.3605
- **Epoch 2**: Training Loss = 0.2922, Validation Loss = 0.3458
- **Epoch 3**: Training Loss = 0.2695, Validation Loss = 0.3501

The training loss shows a steady decrease, indicating effective learning. The validation loss also decreases initially and then stabilizes by the third epoch, suggesting that freezing 5 layers allowed the model to learn effectively while maintaining generalization, without signs of significant overfitting.

With 5 layers frozen in the BART model, the ROUGE and BLEU scores are as follows:
- **ROUGE-1**: 0.2537, showing about 25.4% match in individual words.
- **ROUGE-2**: 0.0733, indicating a 7.3% match in two-word phrases.
- **ROUGE-L** and **ROUGE-Lsum**: 0.1840 and 0.1840, reflecting around 18.4% similarity in sentence structure.

The **BLEU score** is 0.0181, showing a slight improvement in word and phrase alignment compared to previous runs. Freezing only 5 layers, instead of 7, may have allowed the model to learn more effectively from the data, leading to a small but noticeable improvement in overlap with the reference text.
"""