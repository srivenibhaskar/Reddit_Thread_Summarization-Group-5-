# -*- coding: utf-8 -*-
"""model_building_and_evaluation_6E6D_layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6qJFXkQ292nvGffcZeyjHQC_xnqkn2i
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data= pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/updated_cleaned_data.csv")
data.head()

data.info()

df = data.sample(n=5000, random_state=42)

df.info()

model_df = df.copy()

# Specify the relevant columns in the new dataset
relevant_columns = ['document', 'summary']

# Filter the DataFrame to keep only the relevant columns
model_df = model_df[relevant_columns]

# Display the first few rows of the new DataFrame
model_df.head()

#model_df = model_df.rename(columns={'lemmatized_document': 'document', 'lemmatized_summary': 'summary'})

# displaying the first few rows of the updated DataFrame
#model_df.head()

texts = model_df['document'].dropna().tolist()

import os
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments
from torch.utils.data import DataLoader

# loading the pretrained BART tokenizer
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')

# loading the pre-trained BART model
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')

# freezing the first 6 layers of Encoder and Decoder for Transfer Learning
for layer in model.model.encoder.layers[:6]:
    for param in layer.parameters():
        param.requires_grad = False

for layer in model.model.decoder.layers[:6]:
    for param in layer.parameters():
        param.requires_grad = False

# customizing the model for summarization on Reddit dataset
model.config.max_length = 100                                          # max length of generated summaries
model.config.min_length = 20                                           # min length of generated summaries
model.config.no_repeat_ngram_size = 3                                  # to avoid repeating trigrams
model.config.early_stopping = True

# splitting the data into training and validation datasets
from sklearn.model_selection import train_test_split
train_data, eval_data = train_test_split(model_df, test_size=0.2)

import torch
from torch.utils.data import Dataset, DataLoader

class RedditSummaryDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=512):
        self.data = dataframe.reset_index(drop=True)  # Reset index to avoid key errors
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
    # Access the document and summary from the DataFrame
        document = str(self.data.loc[idx, 'document'])
        summary = str(self.data.loc[idx, 'summary'])

    # Tokenize document and summary using the correct tokenizer
        inputs = self.tokenizer(
        document,
        max_length=self.max_length,
        padding='max_length',
        truncation=True,
        return_tensors="pt"
        )
        labels = self.tokenizer(
        summary,
        max_length=self.max_length,
        padding='max_length',
        truncation=True,
        return_tensors="pt"
        ).input_ids

    # Make sure to squeeze the inputs to remove unnecessary dimensions
        inputs['input_ids'] = inputs['input_ids'].squeeze()
        inputs['attention_mask'] = inputs['attention_mask'].squeeze()
        inputs['labels'] = labels.squeeze()  # Ensure labels have the correct shape
        return inputs

# creating datasets for training and evaluation
train_dataset = RedditSummaryDataset(train_data, tokenizer)
eval_dataset = RedditSummaryDataset(eval_data, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=True)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')



# Define the file path in Google Drive
file_path = '/content/drive/My Drive/models/model_frozen_layers.pkl'  # Adjust this path as needed

# Save the model as a pickle file
with open(file_path, 'wb') as file:
    pickle.dump(model, file)

import pickle


file_path = r'C:\Users\kaurd\OneDrive\Desktop\models\model_frozen_layers.pkl'

with open(file_path, 'wb') as file:
    pickle.dump(model, file)
