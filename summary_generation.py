# -*- coding: utf-8 -*-
"""Generating Summaries + Building Model System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17JCX90ilgf06MjmUWjGzvBab4dKXu-lH
"""

from google.colab import drive
drive.mount('/content/drive')

from transformers import BartTokenizer, BartForConditionalGeneration

# Loading the tokenizer
tokenizer = BartTokenizer.from_pretrained('/content/drive/MyDrive/bart_tokenizer')

# Loading the model
model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/bart_model')

import pandas as pd

# Load the file
file_path = '/content/drive/MyDrive/Colab Notebooks/updated_text_analysis.csv'
data = pd.read_csv(file_path)

# Sample 20,000 records from the dataset
sampled_data = data.sample(n=100, random_state=42)

# Select the 'document' column
documents = sampled_data['document'].dropna().tolist()                           # Dropping any NaN values if present

def generate_summary(text):
    try:
        # Tokenize the input with truncation to handle long text
        inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)

        # Generate the summary with controlled parameters
        summary_ids = model.generate(
            inputs['input_ids'],
            max_length=150,
            min_length=40,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )

        # Decode and return the summary text
        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    except Exception as e:
        print(f"Error generating summary for text: {text[:100]}...")             # Print a snippet of the text
        print(f"Error message: {e}")
        return None

from tqdm import tqdm

# Initialize an empty list to store summaries
all_summaries = []

# Define batch size
batch_size = 10

# Process documents in batches with a progress bar
for i in tqdm(range(0, len(documents), batch_size), desc="Processing batches"):
    # Get the current batch of documents
    batch = documents[i:i + batch_size]

    # Generate summaries for the batch
    batch_summaries = [generate_summary(text) for text in batch]

    # Append the batch summaries to the main list
    all_summaries.extend(batch_summaries)

# Add the summaries to the DataFrame
sampled_data['predicted_summary'] = all_summaries

# Save the results to a new CSV file in Google Drive
output_path = '/content/drive/MyDrive/dataset_with_summaries.csv'
sampled_data.to_csv(output_path, index=False)

print("Summaries saved successfully.")
