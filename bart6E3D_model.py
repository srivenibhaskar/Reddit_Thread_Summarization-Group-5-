# -*- coding: utf-8 -*-
"""BART_6E3D_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WBhLhYwNx0tgEKp813HFqWZqCozWoxfa
"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/updated_text_analysis.csv')
df.head(5)

from sklearn.model_selection import train_test_split
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments
import torch
import pandas as pd

# Step 2: Select a smaller subset of the dataset
subset_size = 5000  # Choose the size of your subset
subset_df = df.sample(n=subset_size, random_state=42)  # Randomly select records

# Step 3: Split the dataset into training and testing sets
train_df, test_df = train_test_split(subset_df, test_size=0.2, random_state=42)

# Step 4: Load the BART tokenizer
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

# Function to tokenize the text data
def tokenize_function(examples):
    documents = examples['document'].astype(str).tolist()
    summaries = examples['summary'].astype(str).tolist()

    # Tokenize the documents and summaries
    model_inputs = tokenizer(documents, max_length=1024, truncation=True, padding=True)
    labels = tokenizer(summaries, max_length=150, truncation=True, padding=True)

    model_inputs['labels'] = labels['input_ids']
    return model_inputs

# Tokenize the training and test data
train_encodings = tokenize_function(train_df)
test_encodings = tokenize_function(test_df)

# Step 4: Prepare the data for the Trainer
class SummaryDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = SummaryDataset(train_encodings)
test_dataset = SummaryDataset(test_encodings)

# Step 5: Load the BART model
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

# Step 6: Freeze layers
# Freeze the first 6 layers of the encoder
for layer in model.model.encoder.layers[:6]:  # Access encoder through model.model
    for param in layer.parameters():
        param.requires_grad = False

# Optionally, freeze the first 3 layers of the decoder
for layer in model.model.decoder.layers[:3]:  # Access decoder through model.model
    for param in layer.parameters():
        param.requires_grad = False

# Step 7: Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Step 8: Create Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Step 9: Train the model
trainer.train()

# Optional: Evaluate the model (if needed)
results = trainer.evaluate()
print(results)

"""## Training and Validation Loss Progress
- **Epoch 1-3**:
  - **Training Loss**: Decreases consistently, similar to the first notebook, indicating continued learning.
  - **Validation Loss**: Maintains stability, showing improvement over the first notebook.
"""

# Save the model
model.save_pretrained('/content/drive/MyDrive/bart_6E3D_model')

# Save the tokenizer
tokenizer.save_pretrained('/content/drive/MyDrive/bart_6E3D_tokenizer')