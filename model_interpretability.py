# -*- coding: utf-8 -*-
"""Model_Interpretability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sw6XcUPeWV4efcMGfDx9GLXbtVCT9uCk
"""

import torch
import matplotlib.pyplot as plt
from transformers import BartTokenizer, BartForConditionalGeneration
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# Load the trained BART model and tokenizer
model_path = '/content/drive/MyDrive/bart_6E3D_model'  # Adjust the path to your trained model
tokenizer_path = '/content/drive/MyDrive/bart_6E3D_tokenizer'  # Adjust the path to your trained tokenizer

tokenizer = BartTokenizer.from_pretrained(tokenizer_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

# Load your dataset (adjust the path to your actual dataset)
df = pd.read_csv('/content/drive/MyDrive/updated_text_analysis.csv')
input_text = df['document'][0]  # Adjust the index if needed

# Tokenize the input text
input_tokenized = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Ensure the model is in evaluation mode
model.eval()

# Create a dictionary to store gradients for each layer
layer_gradients = {}

# Define a function to hook into the layers to capture gradients
def capture_gradients(module, grad_input, grad_output):
    layer_gradients[module] = grad_output[0].cpu().detach().numpy()

# Register hooks to capture gradients at each layer of the encoder
for idx, layer in enumerate(model.model.encoder.layers):  # Use 'layers' instead of 'block'
    layer.register_backward_hook(capture_gradients)

# Get embeddings for the input tokens instead of raw token IDs
input_ids = input_tokenized['input_ids']
input_ids = input_ids.to(torch.long)  # Ensure correct dtype for token IDs

# Get embeddings for the tokens
embedding_layer = model.model.shared  # The shared embedding layer
input_embeddings = embedding_layer(input_ids)  # Get the embeddings for the input tokens
input_embeddings.requires_grad_()  # Enable gradient computation on embeddings

# Perform a forward pass with gradient tracking enabled
outputs = model(input_ids=input_ids)

# Compute the loss (using a dummy loss here for simplicity, replace with task-specific loss)
loss = outputs.logits.sum()  # Sum of logits as a placeholder for loss
loss.backward()  # Backpropagate to compute gradients

# Now visualize the gradients for each layer
for idx, gradient in layer_gradients.items():
    # Visualizing the gradient magnitudes for the current layer
    plt.plot(gradient.flatten(), label=f"Layer {idx}")

plt.title("Gradient Magnitudes Across Layers")
plt.xlabel("Token Position")
plt.ylabel("Gradient Magnitude")
plt.legend(loc='upper right')
plt.show()

"""### Layer-wise Gradient Magnitude Insights

- **Early Layers (closer to the input)** have larger gradients: These layers play a more significant role in adjusting the model based on the input.
- **Later Layers (closer to the output)** have smaller gradients: These layers receive less direct influence from the input data and tend to have diminishing gradient magnitudes.

We are freezing the initial layers (the first few layers) of both the encoder and decoder. The early layers are the ones that, according to gradient magnitude analysis, have larger gradients and should therefore be more flexible during fine-tuning.

"""

